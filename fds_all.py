# -*- coding: utf-8 -*-
"""FDS ALL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com
**FDS LAB  ALL ASSIGNMENT CODES**
"""

# -Assignment 1
# -Name: 
'''
- Function Name: mean
- Purpose: To calculate mean of numbers in a list
- Arguments: nums is a list containing integers
- Return Value: Mean value, a float is returned
'''
def mean(nums):
   total = 0.0
   for num in nums:
       total = total + num
   return total/len(nums)

'''
- Function Name: median
- Purpose: To calculate median of numbers in a list
- Arguments: nums is a list containing integers
- Return Value: Median value, a float is returned
'''
def median(nums):
   nums.sort()
   n = len(nums)
   if n % 2 == 1:
      return nums[n//2]
   else:
      return (nums[n//2 - 1] + nums[n//2]) / 2

'''
- Function Name: mode
- Purpose: To calculate mode of numbers in a list.Mode is the number which occurs most frequently
- Arguments: nums is a list containing integers
- Return Value: mode value, a number in the list is returned
'''
def mode(nums):
   frequency = {}
   for num in nums:
       if num in frequency:
           frequency[num] += 1
       else:
           frequency[num] = 1
   max_frequency = max(frequency.values())
   modes = [num for num, freq in frequency.items() if freq == max_frequency]
   return modes

'''
- Function Name: variance
- Purpose: To calculate the variance of numbers in a list
- Arguments: nums is a list containing integers. mean is the mean value of the numbers
- Return Value: variance value, a float is returned
'''
def variance(nums,mean):
   sum = 0
   for num in nums:
       sum += (num - mean)**2
   return sum/(len(nums)-1)

if __name__ == '__main__':
    data = [50,30,20,60,90,10,40,30,10]
    mean = mean(data)
    print("Mean is ",mean)
    print("Median is ",median(data))
    print("Mode is ",mode(data))
    print("Variance is ",variance(data,mean))
    print("Standard Deviation is ",variance(data,mean)**0.5)

'''
Assignment 2: NumPy Exercises
Name: 
'''
import numpy as np
'''
- Function Name: add
- Purpose: To add two 1-D numpy vectors element-wise
- Arguments: a and b are single dimension numpy vectors of type int32
- Return Value: result is a single dimension numpy vector of type int 32
'''

def add(a,b):
    result = np.array([])
    for i in range(a.shape[0]):
        result = np.append(result,a[i] + b[i])
    return result

def dot_product(a,b):
    result = 0
    for i in range(a.shape[0]):
        result = result + a[i]*b[i]
    return result

def mat_add(a,b):
    result = np.zeros(a.shape)
    for i in range(a.shape[0]):
        for j in range(a.shape[1]):
            result[i][j] = a[i][j] + b[i][j]
    return np.array(result)

def mat_mul(a,b):
    result = []
    for i in range(len(a)):
        row = []
        for j in range(len(b[0])):
            res = 0
            for k in range(len(b)):
                res += a[i][k] * b[k][j]
            row.append(res)
        result.append(row)
    return np.array(result)

def check(a,b):
    if a.shape[1] == b.shape[0]:
        return mat_mul(a,b)
    else:
        print("Matrix Cannot be Multiplied")

vec_1 = np.array([1,2,3])
vec_2 = np.array([4,5,6])
vec_3 = np.array([[1,2,5],[5,2,3],[1,1,1]])
vec_4 = np.array([[1,0,0],[0,1,0],[0,0,1]])
print(add(vec_1,vec_2))
print(dot_product(vec_1,vec_2))
print(mat_add(vec_3,vec_4))
print(check(vec_3,vec_4))

# -Assignment 3
# -Name: 

import pandas as pd
import random

# -Importing data from csv files into pandas DataFrame.
df1 = pd.read_csv("part11.csv") #Names1
df2 = pd.read_csv("part12.csv") #Marks1
df3 = pd.read_csv("part21.csv") #Names2
df4 = pd.read_csv("part22.csv") #Marks2

# -To check the definitions of the DataFrames created.
df1.info()
df2.info()
df3.info()
df4.info()

'''
- Merging the 4 DataFrames into a single DataFrame.
- concat method is used here. The DataFrames containing similar data are concatened on axis 0 (vertically).
- The concatenated DataFrames are saved to a new DataFrame.
- Then the 2 DataFrames are concatenated on axis 1 (horizontally) to create a single DataFrame.
'''
merged_pd1 = pd.concat([df1,df3], axis=0, ignore_index=True)
merged_pd2 = pd.concat([df2,df4], axis=0, ignore_index=True)
merged_df = pd.concat([merged_pd1,merged_pd2], axis=1)
merged_df.info()

'''
- Replacing NaN values with zeroes
'''
merged_df.replace('A',0,inplace=True)
print(merged_df)
merged_df.to_csv('merged.csv')
merged_df[['CAT1','CAT2','CAT3']] = merged_df[['CAT1','CAT2','CAT3']].astype('int32')
merged_df.info()

'''
- Summary Statistics
- Adding grace Marks to CAT3
'''
merged_df.describe()
merged_df['CAT3'] = merged_df.apply(lambda row: row['CAT3'] + 5 if row['CAT3'] < 50 else row['CAT3'], axis=1)
merged_df.head()

'''
- Function Name: top_two
- Arguments: l <class-list>
- Purpose: To return the average of 2 maximum values in the list
- Return values: average of 2 max values
- Return Type: float
'''
def average_top_two(l):
    max_1 = l[0]
    max_2 = l[1]
    for i in l:
        if (max_1 < i and max_2 < i):
            max_2 = max_1
            max_1 = i
        elif (max_2 < i and max_1 > i ):
            max_2 = i
    return (max_1 + max_2)/2

'''
- Calculate Internal Marks and merge it into DataFrame
'''
Internal = []
for i in range(len(merged_df)):
    temp = []
    temp.append(merged_df.iat[i,2])
    temp.append(merged_df.iat[i,3])
    temp.append(merged_df.iat[i,4])
    Internal.append(average_top_two(temp))
merged_df['Internal'] = Internal
print(merged_df)
print(merged_df.info())

'''
- The instructor changes her mind about having added grace marks. She wants to wants to undo the addition of grace mark in the earlier step. Instead, she now wants to add a grace mark only to those whose internal mark is below 50.
'''
merged_df.loc[(merged_df['CAT3'] < 55), 'CAT3'] -= 5
merged_df['Internal'] = merged_df.apply(lambda row: row['Internal'] + 5 if row['Internal'] < 50 else row['Internal'], axis=1)
merged_df.head()
'''
- Generate Random values for End Semester and merge it into DataFrame
'''
ESE = []
for i in range(len(merged_df)):
    ESE.append(random.randint(50,100))
merged_df['ESE'] = ESE
print(merged_df)

'''
- Calculate Total Marks and Assign Grade
'''
Total = []
Grade = []
for i in range(len(merged_df)):
    Total.append((int(merged_df.iat[i,5])+int(merged_df.iat[i,6]))/2)
    if Total[i]>=90 and Total[i]<=100:
        Grade.append('O')
    elif Total[i]>=80 and Total[i]<90:
        Grade.append('A')
    elif Total[i]>=70 and Total[i]<80:
        Grade.append('B')
    elif Total[i]>50 and Total[i]<70:
        Grade.append('C')
    else:
        Grade.append('F')
merged_df['Total'] = Total
merged_df['Grade'] = Grade
print(merged_df)
merged_df.to_csv('merged.csv')

"""
Created on Sat Mar 23 10:32:24 2024
@author: vivesh
"""
# %%
import numpy as np
import pandas as pd
import matplotlib as plt
# %%
data_set = pd.read_csv("IPL_Matches_2008_2022.csv")
data_set.info()
# %%
'''
group_wickets = data_set.groupby(["WonBy"])
group_runs = data_set.groupby(["WonBy"])
group_wickets.describe()'''
# %%
df1 = data_set[data_set["WonBy"]== 'Wickets'][['WonBy','Margin']]
df2 = data_set[data_set["WonBy"]== 'Runs'][['WonBy','Margin']]
# %%
print("Mean = ",df1['Margin'].mean())
print("Median = ",df1['Margin'].median())
print("Mode = ",df1['Margin'].mode())
print("Variance = ",df1['Margin'].var())
print("Standard Deviation = ",df1['Margin'].std())
# %%
print("Mean = ",df2['Margin'].mean())
print("Median = ",df2['Margin'].median())
print("Mode = ",df2['Margin'].mode())
print("Variance = ",df2['Margin'].var())
print("Standard Deviation = ",df2['Margin'].std())

#Assignment 5 Sampling
import numpy as np
import matplotlib.pyplot as plt

sample1 = np.random.normal(loc = 100, scale = 15,size = 10).astype('int')
sample2 = np.random.normal(loc = 100, scale = 15,size = 100).astype('int')
sample3 = np.random.normal(loc = 100, scale = 15,size = 1000).astype('int')

mean_s1 = np.mean(sample1)
std_s1 = np.std(sample1)
mean_s2 = np.mean(sample2)
std_s2 = np.std(sample2)
mean_s3 = np.mean(sample3)
std_s3 = np.std(sample3)

print(f"Sample1 mean ={mean_s1}, standard deviation = {std_s1}")
print(f"Sample2 mean ={mean_s2}, standard deviation = {std_s2}")
print(f"Sample3 mean ={mean_s3}, standard deviation = {std_s3}")

n = 5
sample = np.random.normal(loc = 100, scale = 15, size = 5)
print(sample)
print(f"Mean = {np.mean(sample)}")

for i in range(10):
    sample = np.random.normal(loc = 100, scale = 15, size = 5)
    print(sample)
    print(f"Mean = {np.mean(sample)}")

mean_list_s1 = []
mean_list_s2 = []
mean_list_s3 = []
for i in range(10000):
    s1 = np.random.normal(loc = 100, scale = 15, size = 1).astype('int')
    s2 = np.random.normal(loc = 100, scale = 15, size = 2).astype('int')
    s3 = np.random.normal(loc = 100, scale = 15, size = 10).astype('int')
    #print(s1)
    Mean1 = np.mean(s1)
    Mean2 = np.mean(s2)
    Mean3 = np.mean(s3)
    #print(f"Mean = {Mean}")
    mean_list_s1.append(Mean1)
    mean_list_s2.append(Mean2)
    mean_list_s3.append(Mean3)
fig, (axs1, axs2, axs3) = plt.subplots(1,3, figsize=(15, 6))
axs1.hist(mean_list_s1, bins = 50)
axs1.set_xlabel('Mean')
axs1.set_ylabel('Frequency')
axs1.set_title('Histogram n = 1')
axs2.hist(mean_list_s2, bins = 50)
axs2.set_xlabel('Mean')
axs2.set_ylabel('Frequency')
axs2.set_title('Histogram n = 2')
axs3.hist(mean_list_s3, bins = 50)
axs3.set_xlabel('Mean')
axs3.set_ylabel('Frequency')
axs3.set_title('Histogram n = 3')
plt.show()

#Assignment 6 Bootstrap
from scipy.stats import bootstrap
import numpy as np

#convert array to sequence
data = (data,)

#calculate 95% bootstrapped confidence interval for median
bootstrap_ci = bootstrap(data, np.median, confidence_level=0.95,
                         random_state=1, method='percentile')

#view 95% boostrapped confidence interval
print(bootstrap_ci.confidence_interval)

#ASSIGNMENT 7 AND 8
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#Reading the data File
Anor = pd.read_csv('http://stat4ds.rwth-aachen.de/data/Anorexia.dat', sep='\s+')
Anor.head(3)
Anor['change'] = Anor['after'] - Anor['before'] #Creating column change
Anor.loc[Anor['therapy'] == 'cb']['change'].describe() #Summary Statistics of chnage where therapy is cb
#Printing Histogram
bins=list(range(-10,30,2))
plt.hist(Anor.loc[Anor['therapy']=='cb']['change'],bins, edgecolor='k')
plt.xlabel('Weight change'); plt.ylabel('Frequency')
#Calculating Confidence Intervals using stats model
changeCB = Anor.loc[Anor['therapy'] == 'cb']['change']
import statsmodels.stats.api as sms
print(sms.DescrStatsW(changeCB).tconfint_mean())
print(sms.DescrStatsW(changeCB).tconfint_mean(alpha=0.01))

import os
Books = pd.read_csv('http://stat4ds.rwth-aachen.de/data/Library.dat', sep='\s+')
Books.head(3)
Books.describe()
print(np.median(Books['C']))
print(np.median(Books['P']))
plt.boxplot(Books["P"], vert=False) # Box plot of P with outliers
plt.xlabel("Years since publication")
plt.boxplot(Books["P"], vert=False, showfliers=False) #Boxplot without Outliers
plt.xlabel("Years since publication")
import bootstrapped.bootstrap as bs
import bootstrapped.stats_functions as bs_stats
population = Books["P"]
samples = np.array(population[:]) #Taking samples for bootstrapping
print(bs.bootstrap(samples, stat_func=bs_stats.median)) #Bootstrapped median
print(bs.bootstrap(samples, stat_func=bs_stats.std)) #Bootstrapped standard deviation

#ASSIGNMENT 9 HYPOTHESIS TESTING

import pandas as pd
import numpy as np
data = pd. read_csv("ACCIDENTS_GU_BCN_2010.csv", encoding='latin -1')
# Create a new column which is the date
data['Date'] = data['Dia de mes']. apply ( lambda x: str(x)) + '-' + data['Mes de any']. apply ( lambda x: str(x))
data2 = data['Date']
print(data2.head())
counts2010 = data['Date'].value_counts()
print (f'2010: Mean {counts2010. mean()}')
data = pd. read_csv("ACCIDENTS_GU_BCN_2013.csv", encoding='latin -1')
# Create a new column which is the date
data['Date'] = data['Dia de mes']. apply ( lambda x: str(x)) + '-' + data['Mes de any']. apply ( lambda x: str(x))
data2 = data['Date']
counts2013 = data['Date'].value_counts()
print(f'2013: Mean {counts2013. mean()}')
n = len(counts2013)
mean = counts2013. mean()
s = counts2013.std()
ci = [mean - s*1.96/np.sqrt(n), mean + s*1.96/np.sqrt (n)]
print (f'2010 accident rate estimate: {counts2010. mean()}')
print (f'2013 accident rate estimate: {counts2013. mean()}')
print (f'CI for 2013: {ci}')
m = len(counts2010)
n = len(counts2013)
p = (counts2013. mean() - counts2010. mean())
print (f'm: {m}, n: {n}')
print (f'mean difference: ’ {p}')
#Pooling Distribution
x = counts2010
y = counts2013
pool = np. concatenate([x, y])
np.random. shuffle(pool)
import random
N = 10000 # number of samples
diff = range (N)
for i in range (N):
  p1 = np.array([random.choice(pool) for _ in range (n)])
  p2 = np.array([random.choice(pool) for _ in range (n)])
  diff[i] = (np.mean(p1) - np.mean(p2))
  diff2 = np.array(diff)
  w1 = np.where(diff2 > p)[0]
print('p-value ( Simulation)=', len(w1)/ float (N),'(', len(w1)/ float (N)*100 ,'%)', 'Difference =', p)
if ( len(w1)/ float (N)) < 0.05:
  print ('The effect is likely')
else :
  print ('The effect is not likely')

#ASSIGNMENT 10, 11 - MACHINE LEARNING
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

#LINEAR REGRESSION
d_x,d_y = datasets.load_diabetes(return_X_y=True)
#print(d_x)
#print(d_y)
d_x = d_x[:,np.newaxis,2]
print(d_x)
d_x_train = d_x[:-10]
d_x_test = d_x[-10:]
#print(d_x_train)
#print(d_x_test)
d_y_train = d_y[:-10]
d_y_test = d_y[-10:]
#print(d_y_train)
#print(d_y_test)
regr = linear_model.LinearRegression()
regr.fit(d_x_train, d_y_train)
d_y_pred = regr.predict(d_x_test)
print(f"Coefficients: \n {regr.coef_}")
print(f"Mean squared error: {mean_squared_error(d_y_test, d_y_pred)}")
print(f"Coefficient of determination: {r2_score(d_y_test, d_y_pred)}")
plt.scatter(d_x_test, d_y_test, color="black")
plt.plot(d_x_test, d_y_pred, color="blue", linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()

#KNN - NEAREST NEIGHBOUR CLASSIFICATION
from sklearn.neighbors import NearestNeighbors
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
distances, indices = nbrs.kneighbors(X)
print(indices)
print(distances)
nbrs.kneighbors_graph(X).toarray()

#LOGISTIC REGRESSION
from sklearn. linear_model import LogisticRegression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd. read_csv('SP1.csv')
s = data[[ 'HomeTeam','AwayTeam', 'FTHG', 'FTAG', 'FTR']]

def my_f1(row):
  return max(row['FTHG'], row['FTAG'])
def my_f2(row):
  return min(row['FTHG'], row['FTAG'])

s ['W'] = s. apply (my_f1 , axis = 1)
s ['L'] = s. apply (my_f2 , axis = 1)
x1 = s['W'].values
y1 = np.ones( len(x1), dtype = np.int32)
x2 = s['L'].values
y2 = np.zeros( len(x2), dtype = np.int32)
x = np.concatenate ([x1 , x2])
x = x[:, np.newaxis]
y = np.concatenate ([y1 , y2])

logreg = LogisticRegression()
logreg.fit(x, y)
X_test = np.linspace(-5, 10, 300)

def lr_model (x):
  return 1 / (1+np.exp(-x))

loss = lr_model(X_test*logreg.coef_ + logreg. intercept_).ravel()
X_test2 = X_test[:,np.newaxis]
losspred = logreg.predict(X_test2)
plt.scatter(x.ravel(), y,color = 'black',s = 100, zorder = 20,alpha = 0.03)
plt.plot(X_test , loss , color = 'blue', linewidth = 3)
plt.plot(X_test , losspred , color = 'red', linewidth = 3)

#CIA 2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#Question 1
df = pd.read_csv("ipl.csv") #Read csv file
df1 = df[['WinningTeam', 'Margin', 'WonBy']] #selecting required columns
df1 = df1[df1['WonBy'] == 'Runs'] #rearranging data
mean_margin = df1['Margin'].mean() #Finding mean using mean function
#print(df1.head())
print(f"Mean = {mean_margin}")
#Plotting Distribution
x = df1['Margin']
plt.hist(x,bins = 30)
plt.xlabel('Margins')
plt.ylabel('Frequency')
plt.title('Distribution')
plt.show()
#Question 2
#Finding Confidence Intervals using scipy.stats
import scipy.stats as stats
std_margin = df1['Margin'].std()

x = [0.10,0.05,0.01]
for i in x:
  z = stats.norm.ppf(1- i/2)
  ci = (mean_margin - z * (std_margin / (len(df1['Margin']) ** 0.5)),
         mean_margin + z * (std_margin / (len(df1['Margin']) ** 0.5)))
  print(f"90% Confidence Interval: {ci}")
  #Question 3
#Confidence Interval by Bootstrapping
from scipy.stats import bootstrap
data = (df1['Margin'],)

print("Confidence Intervals using Bootstrap")

x = [0.90,0.95,0.99]
for i in x:
  bootstrap_mean = bootstrap(data,np.mean, confidence_level = i, random_state=1, method='percentile')
  print(f"{i}% {bootstrap_mean.confidence_interval}")
  #Question 4: Null Hypothesis
#NULL HYPOTHESIS H0:Let Mean margin of victory is 27
#We accept null hypothesis if mean is ablove 27
#else we reject null hypothesis

h0 = 27
standard_error = np.std(df1['Margin']) / np.sqrt(len(df1['Margin']))
x = [0.10,0.05,0.01]
for i in x:
  z = (mean_margin - h0)/standard_error
  p = 2 * (1 - stats.norm.cdf(abs(z)))
  if p < i:
    print(f"Reject the null hypothesis at the {i}% significance level.")
  else:
    print(f"Accept the null hypothesis at the {i}% significance level. ")
